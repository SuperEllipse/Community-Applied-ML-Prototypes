name: Community

entries:
  - title: Contextual Chatbot with NeMo Guardrails
    label: CML_AMP_NeMo-Guardrails-Chatbot
    short_description: |
        This Applied Machine Learning Prototype (AMP) is a similarity-search chatbot that demonstrates safe and responsible AI use for organizations through customizable guardrails.
    long_description: |
        This Applied Machine Learning Prototype (AMP) builds a similarity-search based chatbot built using Langchain, OpenAI embeddings, Pinecone Vector DB, and NeMo-Guardrails. This chatbot is designed to showcase how organizations can leverage AI safely and responsibly by implementing guardrails.
    long_description_html: |
        This Applied Machine Learning Prototype (AMP) builds a similarity-search based chatbot built using Langchain, OpenAI embeddings, Pinecone Vector DB, and NeMo-Guardrails. This chatbot is designed to showcase how organizations can leverage AI safely and responsibly by implementing guardrails.
    image_path: >-
      https://raw.githubusercontent.com/kevinbtalbert/CML_AMP_NeMo-Guardrails-Chatbot/main/assets/demo.png
    tags:
      - NeMo Guardrails
      - Secure AI
      - OpenAI
      - Langchain
      - Pinecone
      - Streamlit
      - NVIDIA Rails
      - Chatbot
    git_url: 'https://github.com/kevinbtalbert/CML_AMP_NeMo-Guardrails-Chatbot.git'
    is_prototype: true
    is_community: true
    is_new: true
  - title: CML HuggingFace Models
    label: cml_hf_models
    short_description: |
        Choose any 7B or 13B LLM from HuggingFace and deploy as a CML Model.
    long_description: |
        Choose any 7B or 13B LLM from HuggingFace and deploy as a CML Model. Cloudera Machine Learning models expose an Inference endpoint for users to access and communicate with. The AMP creates a Gradio App UI which can be used to interact with the deployed CML Model.
    long_description_html: |
        Choose any 7B or 13B LLM from HuggingFace and deploy as a CML Model. Cloudera Machine Learning models expose an Inference endpoint for users to access and communicate with. The AMP creates a Gradio App UI which can be used to interact with the deployed CML Model.
    image_path: >-
      https://raw.githubusercontent.com/nkityd09/cml_hf_models/main/images/cml_hf_ui.png
    tags:
      - huggingface
      - 7B
      - 13V
    git_url: 'https://github.com/nkityd09/cml_hf_models.git'
    is_prototype: true
    is_community: true
    is_new: true
  - title: Text to Image Using Stable Diffusion
    label: CML_AMP-Text-to-Image-with-Stable-Diffusion
    short_description: |
        Run a browser interface based on Gradio library for Stable Diffusion within the CML platform.
    long_description: |
        Run a browser interface based on Gradio library for Stable Diffusion within the CML platform.
    long_description_html: |
        Run a browser interface based on Gradio library for Stable Diffusion within the CML platform.
    image_path: >-
      https://raw.githubusercontent.com/kevinbtalbert/CML_AMP-Text-to-Image-with-Stable-Diffusion/master/catalog-entry.png
    tags:
      - Text2Image
      - Stable Diffusion
    git_url: 'https://github.com/kevinbtalbert/CML_AMP-Text-to-Image-with-Stable-Diffusion.git'
    is_prototype: true
    is_community: true
    is_new: true
  - title: Text Summarization using IBM watsonx.ai
    label: CML_AMP_watsonxai
    short_description: |
        This repository demonstrates how to use watson machine learning Python SDK to call watsonx.ai models from Cloudera Machine Learning (CML) workspace.
    long_description: |
        This repository demonstrates how to use watson machine learning Python SDK to call watsonx.ai models from Cloudera Machine Learning (CML) workspace.
    long_description_html: |
        This repository demonstrates how to use watson machine learning Python SDK to call watsonx.ai models from Cloudera Machine Learning (CML) workspace.
    image_path: >-
      https://raw.githubusercontent.com/agupta-git/CML_AMP_watsonxai/main/assets/app_interface.png
    tags:
      - watsonx
      - Text Summarization
      - IBM
    git_url: 'https://github.com/agupta-git/CML_AMP_watsonxai.git'
    is_prototype: true
    is_community: true
    is_new: true
  - title: Ray on CML QuickStart
    label: ray
    short_description: A series of starter notebooks that demonstrate how to use Ray on CML
    long_description: >-
      A series of starter notebooks that demonstrate how to launch a Ray cluster, use python libraries, train and deploy a model using Ray Tune in CML.
    image_path: >-
      https://github.com/vidushisomani/CML_Ray_Starter_AMP/blob/main/images/amp-cover.png?raw=true
    tags:
      - Ray
    git_url: "https://github.com/vidushisomani/CML_Ray_Starter_AMP.git" 
    is_prototype: true
    is_community: true
    is_new: true
  - title: Solr 9
    label: CML_AMP_solr-runtime
    short_description: |
      Run Solr 9 as an Application within an AMP. Installs necessary Solr and Java runtime components.
    long_description: |
      Run Solr 9 as an Application within an AMP. Installs necessary Solr and Java runtime components. By default, Solr 9.3.0 and Java 11.0.1 will be used.
    image_path: "https://raw.githubusercontent.com/kevinbtalbert/CML_AMP_Solr_9/main/assets/amp-cover.png"
    tags: 
      - Solr 9
      - Vector DB
    git_url: "https://github.com/kevinbtalbert/CML_AMP_Solr_9.git"
    is_prototype: true
    is_community: true
    is_new: true
  - title: Mistral 7B CML-Hosted Model
    label: llm-model-deploy
    short_description: |
      This AMP deploys Mistral-7B model as a CML API endpoint. The "Ephemeral Storage Limit" in Site Administration must be set to 20GB or greater before deploying this AMP.
    long_description: |
      This AMP deploys Mistral-7B model as a CML API endpoint. Requires a GPU node with 4 vCores and 16 GB memory minimum. Note, you will need to ensure the "Ephemeral Storage Limit" in Site Administration is set to 20GB or greater for the model to successfully deploy. IMPORTANT: Please read the following before proceeding.  This AMP includes or otherwise depends on certain third party software packages.  Information about such third party software packages are made available in the notice file associated with this AMP.  By configuring and launching this AMP, you will cause such third party software packages to be downloaded and installed into your environment, in some instances, from third partiesâ€™ websites.  For each third party software package, please see the notice file and the applicable websites for more information, including the applicable license terms. If you do not wish to download and install the third party software packages, do not configure, launch or otherwise use this AMP.  By configuring, launching or otherwise using the AMP, you acknowledge the foregoing statement and agree that Cloudera is not responsible or liable in any way for the third party software packages.
    image_path: "https://raw.githubusercontent.com/cloudera/CML_AMP_Deploy-Mistral7B-CML-Native-Model/main/images/catalog-entry.png"
    tags: 
      - Mistral 7B
      - LLM
      - CML Labs
      - Model Deployment
      - GPU
    git_url: "https://github.com/cloudera/CML_AMP_Deploy-Mistral7B-CML-Native-Model.git"
    is_prototype: true
    is_community: true
    is_new: true
  - title: WealthE
    label: WealthE
    short_description: |
      WealthE is a GenAI wealth education assistant, which uses llama3 & Ollama to answer queries on Indian Capital Markets. 
    long_description: |
      This AMP uses Ollama LLM service to create a self-hosted Wealth Educator application contextualized for Indian Capital Market.It uses the llama3 LLM to provide ChatGPT style interface that holds conversational memory. In addition we demonstrate LLM Ops through prompt evaluation using TruLens and Mlflow frameworks.
    long_description_html: |
      This AMP uses Ollama LLM service to create a self-hosted Wealth Educator application contextualized for Indian Capital Market.It uses the llama3 LLM to provide ChatGPT style interface that holds conversational memory. In addition we demonstrate LLM Ops through prompt evaluation using TruLens and Mlflow frameworks.
    image_path: >-
      https://raw.githubusercontent.com/superellipse/WealthE/main/assets/images/amp-logo.png    
    tags:
      - LLMOps
      - ollama
      - llamaindex
      - llama3
      - chainlit
      - Trulens
    git_url: 'https://github.com/SuperEllipse/WealthE'
    is_prototype: true
    is_community: true
    is_new: true